{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Q7WKU4_Modafar_MTM_task_solution_Report.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1UiKXbDvYSDdDTm0EWlqFqAtoU_Vzp8X4","authorship_tag":"ABX9TyNUa4njo4ctsGTGOgKE1mho"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zeiwSnvZQwsP"},"source":["# Abstract"]},{"cell_type":"markdown","metadata":{"id":"RergLgtgQ1IF"},"source":["In this report, the final task solution is documented and detailed. The document walks the reader through the task descriptions and requirmenets first. After that, it presents the task solution starting from the data preparation and preprocessing, then the model design and training, and at the end it presents the test predictions preparation. In addition to this, there is an appendix where all the results and curves are shown."]},{"cell_type":"markdown","metadata":{"id":"z1AxDZ7RRDiH"},"source":["# Task description"]},{"cell_type":"markdown","metadata":{"id":"R0HaaI5rRKQO"},"source":["## Requirments"]},{"cell_type":"markdown","metadata":{"id":"a8MyTFsJRQVB"},"source":["In this task, a classifier needs to be trained to perform classification task on black and white images in order to identify the character in each image out of 62 different classes. The following requirements have to be fulfilled:\n","1.\tApproach:\n","A deep learning classifier model has to be designed, trained and optimized following cross validation approach, and the results of the training and testing have to be saved.\n","2.\tIndicators:\n","The main two goodness indicators/metrics which have to be used are accuracy and area under curve “AUC”.\n","3.\tOutput format:\n","Text file contains the prediction for the testing unlabeled data, each preditction is in a separate line/row in the corresponding order of the provided data.\n"]},{"cell_type":"markdown","metadata":{"id":"3lA6Gkm8RV7I"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"SqRutRLzRXl1"},"source":["The provided data structure and features are as the following:\n","1.\tDataset splits:\n","The data is split into two main folders, training and testing. The testing folder contains the testing data, where there are 7100 images, each is named with respect to its order. This data is without labels, since it is used for the overall task evaluation by the teacher. The prediction submission file content is created based on this data.\n","The other folder, i.e. training data folder, contains around 53172 images which are distributed over 62 subdolders, each folder corresponds to a separate class/label. This data is used for training, validation and performance evaluation.\n","2.\tDataset description:\n","The data is black and white character images, each image is of size (128 x 128 x 3). Data has 62 different categories. The first ten classes (1 - 10) are for digits characters (0 – 9), capital letters (A – Z) are assigned classes (11 – 36), and lastly the small letters characters (a – z) have the classes (37 – 62).\n"]},{"cell_type":"markdown","metadata":{"id":"0iYhTRezSVRl"},"source":["# Task solution"]},{"cell_type":"markdown","metadata":{"id":"kldEPXGXSY7o"},"source":["## Data preperation"]},{"cell_type":"markdown","metadata":{"id":"nMgxj9uvSjp0"},"source":["The main objective of this step is to record the images and their labels in a reusable format, to avoid repeating data gathering and labels assignment. Another additional step is to compress the data in a proper data structure where data importing at the beginning of the training can be easy. An extra motivation for the data processing is the limitations of the computational and storage resources. To do so, the following steps are performed:"]},{"cell_type":"markdown","metadata":{"id":"V5alG-yTTIBu"},"source":["### Data and labels reading"]},{"cell_type":"markdown","metadata":{"id":"jhk2j18zTNwY"},"source":["All training images are read and stacked in an np array, and another array is built for their corresponding labels which are extracted from the from the file/folder name . The same is done for the testing data, the only difference that there are no labels to extract.\n","Also, and additional dictionary is built to match between the class number and the characters, e.g. class number ‘11’ corrosponds to character ‘A’."]},{"cell_type":"code","metadata":{"id":"v_leE5AgTAk1"},"source":["train_dir = './Train/*/*.png'\n","train_imgs, train_labels = [], []\n","\n","for im_dir in glob.glob(train_dir):\n","  im = cv2.imread(im_dir)\n","  idx = im_dir.index('img')\n","  im_class = int(im_dir[idx-3:idx-1])\n","  train_imgs.append(im)\n","  train_labels.append(im_class)\n","print(len(train_imgs))\n","train_imgs = np.stack(train_imgs)\n","print(train_imgs.shape)\n","print(len(train_labels))\n","train_labels = np.stack(train_labels)\n","print(train_labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2noCIbcTa3P"},"source":["numbers = [str(i) for i in range(0,10)]\n","upper_case_letters = list(string.ascii_uppercase)\n","lower_case_letters = list(string.ascii_lowercase)\n","digits = numbers + upper_case_letters + lower_case_letters\n","\n","digits_dict = {}\n","\n","for idx in range(len(digits)):\n","  digits_dict[idx+1] = digits[idx]\n","\n","classes = np.array(list(digits_dict.items()))\n","\n","print(classes.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxaG1r2PTfou"},"source":["### Compressing"]},{"cell_type":"markdown","metadata":{"id":"YBNfWfSwTmQh"},"source":["Both training and testing data are compressed into ‘train_data.npz’ and ‘test_data.npz’ respectively using “compressArray” function."]},{"cell_type":"markdown","metadata":{"id":"CeB5WJTdThyu"},"source":["### Uncompressing"]},{"cell_type":"markdown","metadata":{"id":"Ag0OiwvCTqyd"},"source":["Later, the compressed files are un compressed into np arrays (for the data) and a dictionary (for the classes) using “uncompressArray” function."]},{"cell_type":"code","metadata":{"id":"3AOIM8ppTtjZ"},"source":["def compressArray(file_dir, images=None, labels=None, classes=None, train=True):\n","  with open(file_dir, 'wb') as file:\n","    if train:\n","      np.savez_compressed(file, x_train=images, y_train=labels, classes=classes)\n","    else:\n","      np.savez_compressed(file, x_test=images)\n","        \n","def uncompressArray(file_dir, train=True):\n","  key = 'x_train' if train else 'x_test'\n","  with open(file_dir, 'rb') as file:\n","    loaded_file = np.load(file)\n","    images = loaded_file[key].copy()\n","    labels, classes = None, None\n","    if train:\n","      labels = loaded_file['y_train'].copy()\n","      classes = dict(loaded_file['classes'].copy())\n","  return images, labels, classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioNVdjabUAz-"},"source":["## Packages importing"]},{"cell_type":"markdown","metadata":{"id":"kLrgeP1_UE9V"},"source":["The first step in the training is to import the needed packages, some of them are needed for model building, some are needed for training, and others for data processing."]},{"cell_type":"code","metadata":{"id":"dI5SqIi-40cv"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation,\\\n","                         Conv2D, MaxPooling2D, Flatten\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bt7mV_QMUVEe"},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"s54yLe68Ub5N"},"source":["The second important step is to perform data preprocessing, this includes importing and processing the data to prepare it for training stage."]},{"cell_type":"markdown","metadata":{"id":"axzdvgOEUgCg"},"source":["### Data importing"]},{"cell_type":"markdown","metadata":{"id":"BCYQJShVUrnF"},"source":["The training data and labels are imported into np arrays. The data and labels are in matching order."]},{"cell_type":"code","metadata":{"id":"bboBn1yMUmw4"},"source":["train_file = './data/train_data.npz'\n","X_train, Y, classes = uncompressArray(train_file)\n","X_train.shape, Y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RrGp-hnkU2Wm"},"source":["### Image resizing"]},{"cell_type":"markdown","metadata":{"id":"vtaQ6kYIU517"},"source":["Due to the limitations of the computational resources, the images are resized from 128x128x3 into 32x32x3. Of course, smaller images leads to less processing resources and memory are required."]},{"cell_type":"code","metadata":{"id":"aeDqDse_VLMY"},"source":["X = []\n","WIDTH, HEIGHT = 32, 32\n","for image in X_train:\n","  X.append(cv2.resize(image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n","X = np.asarray(X)\n","del X_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96oadXaOVOy3"},"source":["### Data splitting"]},{"cell_type":"markdown","metadata":{"id":"GHZfH7AIVSZv"},"source":["Next, data is split and shuffled into train and test data (note that test here means the training test data, not the task evaluation unlabeled data). 80% of the data is used for training and the remaining is for testing."]},{"cell_type":"code","metadata":{"id":"_mhwzHpzVZM2"},"source":["X, x_test, Y, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRXCH_xHVZ5G"},"source":["### Data normalization"]},{"cell_type":"markdown","metadata":{"id":"E37PpIxzVgQj"},"source":["Till this point, the pixels values in the images are in range of (0 – 255), before going further these values are normalized for training and testing images, the pixel value new range is (0 – 1).\n","An extra processing is performed here again in order to reduce the computational cost of the data processing. Only one channel i extracted of each image out of the available three. This is done since the images are black and white, which means that the three channels are the same, and one channel contains the same information of the three. Each image becomes of size 32x32 instead 32x32x3 (notice that the last dimension is removed)."]},{"cell_type":"code","metadata":{"id":"3ZeIJjblVdle"},"source":["X = X[:,:,:,0]/255.\n","x_test = x_test[:,:,:,0]/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hj-o2_ZQV25u"},"source":["### Data reshaping"]},{"cell_type":"markdown","metadata":{"id":"cAEOuTh8V6mO"},"source":["After the previous step processing, the lost dimension need to be returned while keeping only one channel for each image. This is done by reshaping the np arrays."]},{"cell_type":"code","metadata":{"id":"9bwoHXumWExW"},"source":["def reshape_data(X):\n","  X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n","  return X\n","\n","X = reshape_data(X)\n","x_test = reshape_data(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCe1OnKTWIke"},"source":["### One hot encoding"]},{"cell_type":"markdown","metadata":{"id":"-wUiKVnhWN0X"},"source":["In this step the labels are vectorized, i.e. one hot encoding representation. However, since the labels start from ‘1’, and since the desired representation should contain the same number of classes, ‘1’ is subtracted from the labels when encoding them . An important point, that at this stage only test labels are processed, while for training labels the processing is performed inside the cross validation loop for each fold separately."]},{"cell_type":"code","metadata":{"id":"LhxyaLfiWYF2"},"source":["y_test = to_categorical(y_test-1, dtype =\"uint8\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMkvhP4SWYw-"},"source":["## Model design"]},{"cell_type":"markdown","metadata":{"id":"vwtbe6OwWi0l"},"source":["In order to build a classification model, a deep sequentional model is designed. The design contains convolutional, activation and dense layers."]},{"cell_type":"markdown","metadata":{"id":"JBvKSOrsWn0N"},"source":["### Model architecture"]},{"cell_type":"markdown","metadata":{"id":"65GIEOoKWy6_"},"source":["The model consists of sequentional layers. The input layer is a Conv layer with Relu activation, which receives the image, then it passes it another Conv layer with Relu activation of the same size, after passing the result to a Maxpooling layer, 25% of the nodes are dropped. The output from this step is sent to upsampling step, which is similar to the previous one, except that it is with a doubled size. The last step is to make the prediction, to do so, the network is flattened and a Dense layer is added, again Relu activation is used, and 50% of the nodes are dropped here. Lastly it is connected to a Dense layer of size of the number of classes in order to make the predictions, while the activation function which is used here is SoftMax."]},{"cell_type":"markdown","metadata":{"id":"5DadC5MOXJTv"},"source":["### Model parameters"]},{"cell_type":"markdown","metadata":{"id":"atIEFL-IXMTf"},"source":["In overall, the model has 2,194,462 trainable parameters."]},{"cell_type":"code","metadata":{"id":"5C1PBug9XXRZ"},"source":["def classificationModel(input_shape, num_classes):\n","  model = Sequential()\n","\n","  model.add(Conv2D(32, (3,3), padding='same', activation=\"relu\", input_shape=input_shape))\n","  model.add(Conv2D(32, (3,3), padding='same', activation=\"relu\"))\n","  model.add(MaxPooling2D(pool_size=(2,2)))\n","  model.add(Dropout(0.25))\n","  \n","  model.add(Conv2D(64, (3,3), padding='same', activation=\"relu\"))\n","  model.add(Conv2D(64, (3,3), padding='same', activation=\"relu\"))\n","  model.add(MaxPooling2D(pool_size=(2,2)))\n","  model.add(Dropout(0.25))\n","  \n","  model.add(Flatten())\n","  model.add(Dense(512))\n","  model.add(Activation('relu'))\n","  model.add(Dropout(0.5))\n","  model.add(Dense(num_classes))\n","  model.add(Activation('softmax'))\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-kzxRWKXXRZ","executionInfo":{"status":"ok","timestamp":1637754490731,"user_tz":-60,"elapsed":3032,"user":{"displayName":"Modafar Al-Shouha","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08841682443665180805"}},"outputId":"d9f7b71e-ba81-4f44-b61d-3333f342f917"},"source":["input_shape = (32,32,1)\n","no_classes = 62\n","Model = classificationModel(input_shape, no_classes)\n","Model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 32, 32, 32)        320       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 16, 16, 32)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n","                                                                 \n"," flatten (Flatten)           (None, 4096)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               2097664   \n","                                                                 \n"," activation (Activation)     (None, 512)               0         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 62)                31806     \n","                                                                 \n"," activation_1 (Activation)   (None, 62)                0         \n","                                                                 \n","=================================================================\n","Total params: 2,194,462\n","Trainable params: 2,194,462\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"ubfx6znlXlyR"},"source":["## Model training"]},{"cell_type":"markdown","metadata":{"id":"MuxdplBMXwkK"},"source":["For the training step, the cross validation approach is performed, and the training parameters are optimized to achieve the best performance. During the training the best model is saved, and it is the one which is used to predict the (unlabeled) test data."]},{"cell_type":"markdown","metadata":{"id":"kt_YhtobX1U8"},"source":["### Tarining paramters\n","a.\tBatch size: 32\n","b.\tNumber of epochs: 20\n","c-\tInput shape: (32, 32, 1)\n","d-\tLoss function: Categorical Cross Entropy (CCE)\n","e-\tOptimizer: Adam\n","f-\tMetrics: Accuracy and Area Under Curve (AUC)\n","g-\tNumber of folds (for cross validation): 5"]},{"cell_type":"code","metadata":{"id":"1bPkfiy4YaKO"},"source":["Batch_size = 32\n","no_epochs = 20\n","input_shape = (32,32,1)\n","no_classes = 62\n","Loss_function = 'Categorical Cross Entropy'\n","Optimizer = 'Adam'\n","metrics=['accuracy', 'auc']\n","no_folds = 5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lr8Oid14Y_Lb"},"source":["### Cross validation"]},{"cell_type":"markdown","metadata":{"id":"n_AOhovCZGoC"},"source":["As mentioned in the training parameters, the number of chosen folds is 5, this means that the training is performed on five diffenet sets of training and validation data. For the validation data, it is chosen by the fold function. After every fold, the resulted model performance is tested against the test data which was held out previously, and the best model is saved."]},{"cell_type":"code","metadata":{"id":"ee2CQnGhxxVT"},"source":["# Model configuration\n","batch_size = 32\n","img_width, img_height, img_num_channels = X.shape[1:]\n","loss_function = 'categorical_crossentropy'\n","no_epochs = 20\n","optimizer = 'adam'\n","verbosity = 1\n","num_folds = 5\n","no_classes = 62\n","\n","# Determine shape of the data\n","input_shape = (img_width, img_height, img_num_channels)\n","\n","# Define per-fold score containers\n","acc_per_fold = []\n","loss_per_fold = []\n","auc_per_fold = []\n","best_model = None\n","best_fold = 0\n","best_acc = 0\n","\n","# Define the K-fold Cross Validator\n","kfold = StratifiedKFold(n_splits=num_folds, shuffle=False)\n","\n","# K-fold Cross Validation model evaluation\n","fold_no = 1\n","for train, test in kfold.split(X, Y):\n","\n","  fold_dir = os.path.dirname(f'./Folds/{fold_no}/')\n","\n","  Y_train = to_categorical(Y[train]-1, dtype =\"uint8\")\n","  Y_test = to_categorical(Y[test]-1, dtype =\"uint8\")\n","\n","  # Define the model architecture\n","  model = classificationModel(input_shape, no_classes)\n","\n","  # Compile the model\n","  model.compile(loss=loss_function,\n","                optimizer=optimizer,\n","                metrics=['accuracy', tf.keras.metrics.AUC(name='auc', multi_label=True)])\n","  \n","  # Include the epoch in the file name (uses `str.format`)\n","  checkpoint_file = 'cp-{epoch:02d}.ckpt'\n","  checkpoint_path = os.path.join(fold_dir, checkpoint_file)\n","\n","  # Create a callback that saves the model's weights every 5 epochs\n","  cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n","                                                    verbose=2, \n","                                                    save_weights_only=True,\n","                                                    save_freq='epoch',\n","                                                    period=5)\n","\n","  model.save_weights(checkpoint_path.format(epoch=0))\n","\n","  # Generate a print\n","  print('------------------------------------------------------------------------')\n","  print(f'Training for fold {fold_no} ...')\n","\n","  # Fit data to model\n","  history = model.fit(X[train], Y_train,\n","              batch_size=batch_size,\n","              epochs=no_epochs,\n","              verbose=verbosity, \n","              callbacks=[cp_callback],\n","              validation_data=(X[test], Y_test))\n","  \n","  # save plots\n","  savePlots(history, fold_dir)\n","  # save history to csv: \n","  hist_csv_file = f'history_{fold_no}.csv'\n","  hist_csv_path = os.path.join(fold_dir, hist_csv_file)\n","  # convert the history.history dict to a pandas DataFrame:     \n","  hist_df = pd.DataFrame(history.history)\n","\n","  with open(hist_csv_path, mode='w') as f:\n","    hist_df.to_csv(f)\n","\n","  scores = model.evaluate(x_test, y_test, verbose=0)\n","  current_acc = scores[1]*100\n","  current_loss = scores[0]\n","  current_auc = scores[2]\n","\n","  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {current_loss}; {model.metrics_names[2]} of {current_auc}; {model.metrics_names[1]} of {current_acc}%')\n","  \n","  acc_per_fold.append(current_acc)\n","  loss_per_fold.append(current_loss)\n","  auc_per_fold.append(current_auc)\n","\n","  if (fold_no == 1) or (current_acc > best_acc):\n","    best_fold = fold_no\n","    best_model = model\n","    best_acc = current_acc\n","\n","  # Increase fold number\n","  fold_no = fold_no + 1\n","\n","best_model.save('./best_model/best_model.h5')\n","# acc_per_fold = acc_per_fold[1:]\n","# == Provide average scores ==\n","print('------------------------------------------------------------------------')\n","print('Score per fold')\n","for i in range(0, len(acc_per_fold)):\n","  print('------------------------------------------------------------------------')\n","  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}% - AUC: {auc_per_fold[i]}')\n","print('------------------------------------------------------------------------')\n","print('Average scores for all folds:')\n","print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n","print(f'> Loss: {np.mean(loss_per_fold)}')\n","print(f'> AUC: {np.mean(auc_per_fold)}')\n","print('------------------------------------------------------------------------')\n","print('Best fold of all folds:')\n","print(f'> Fold: {best_fold}')\n","print('------------------------------------------------------------------------')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aGp1F5huZPSa"},"source":["### Best Model"]},{"cell_type":"markdown","metadata":{"id":"7H_6eWmFZW8i"},"source":["The best model achieved accuracy is 92.77% and 91.97% for training and testing respectively.\n","\n","|| Accuracy | Loss | AUC |\n","| --- | --- | --- | --- |\n","|Training| 92.77%\t| 0.1789 | 0.9987 |\n","|Validation| 91.81%\t| 0.2202 | 0.9973 |\n","|Testing| 91.97% | 0.2207 | 0.9968 |\n","\n","The training and validation curves, show smooth learning behviour and proper convergence. Also, it appears that after the 10th epoch, the learning process started to stabilize. Higher accuracy might have been achieved with more epochs, but the risk of overfitting increases.\n","\n","<img src='https://drive.google.com/uc?id=12HnGtmbuCtOlaOMq2lE5ZrNJLNlnqm0i'>\n","<img src='https://drive.google.com/uc?id=12HPcTTe_zH6l-aKCJz3M8KNM9C4RGGL8'>\n","<img src='https://drive.google.com/uc?id=12H6oxNEIyLYDs0ogfF7xkHJi-xjPPnLM'>"]},{"cell_type":"markdown","metadata":{"id":"qVbVlhCsawYZ"},"source":["## Test data prediction"]},{"cell_type":"markdown","metadata":{"id":"tV5Qy-vojIVf"},"source":["The best model is used to predict the classes/labels of the provided (unlabeled) test data, and the results/predictions are prepared for submission."]},{"cell_type":"markdown","metadata":{"id":"jtUDn_C3jPay"},"source":["### Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"u-0SJg1SjazR"},"source":["The same steps which are perfomed for the training step are performed here except, i.e. importing, resizing, normalization and reshaping. Note here that neither splitting is not needed, nor the one hot encoding (because there are no labels)."]},{"cell_type":"code","metadata":{"id":"UaowI32lUMPy"},"source":["# Read test data\n","test_file = './data/test_data.npz'\n","X_test, _, _ = uncompressArray(test_file, train=False)\n","\n","# Preprocess test data\n","# Resize\n","X_TEST = []\n","WIDTH, HEIGHT = 32, 32\n","for image in X_test:\n","  X_TEST.append(cv2.resize(image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n","X_TEST = np.asarray(X_TEST)\n","del X_test\n","# Normalize\n","X_TEST = X_TEST[:,:,:,0]/255. # extract only one channel\n","# Reshape\n","def reshape_data(X):\n","  X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n","  return X\n","X_TEST = reshape_data(X_TEST)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gruR1FJ4jXsD"},"source":["### Model loading"]},{"cell_type":"markdown","metadata":{"id":"6URC2rGHkCQi"},"source":["The best model is loaded in order to be used for test data classes predicting."]},{"cell_type":"code","metadata":{"id":"a04sdKUKkrY0"},"source":["Best_model = tf.keras.models.load_model('./best_model/best_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GH6FmQ5skypK"},"source":["### Predictions generation"]},{"cell_type":"markdown","metadata":{"id":"sddzM9gpk4O_"},"source":["The predictions are generated in one hot encoding format, then ArgMax is used to return the class/label, here ‘1’ is added to the predicted label, because the predictions are generated based on the training data where ‘1’ was subtracted of the labels values. The final predictions are added to a predictions list, then converted to a dictionary which contain the test iamge file names as keys, and their corrosponding predictions as values."]},{"cell_type":"code","metadata":{"id":"SuHcnYntkrV3"},"source":["OneHot_predictions = Best_model.predict(X_TEST)\n","\n","predictions = [np.argmax(prediction)+1 for prediction in OneHot_predictions] # Labels start from 1\n","\n","pred_dict = {}\n","for idx, prediction in enumerate(predictions):\n","  key = f'Test{(idx+1):04d}.png'\n","  value = str(prediction)\n","  pred_dict[key] = value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mGwaQyYhldGA"},"source":["### Creating submission file"]},{"cell_type":"markdown","metadata":{"id":"t36H9mVBljQW"},"source":["Lastly, submission file is created by writing the predictions dictionary content into a txt file, each prediction in separate row/line, in the following format:\n","*class;TestImage*."]},{"cell_type":"code","metadata":{"id":"8-1NousIUMM4"},"source":["results_dir = os.path.dirname('./submission_results/')\n","results_file = 'submission.txt'\n","results_path = os.path.join(results_dir, results_file)\n","\n","with open(results_path, 'w') as f:\n","  f.write('class;TestImage\\n')\n","  for key, value in pred_dict.items():\n","    line = f'{value};{key}\\n'\n","    f.write(line)"],"execution_count":null,"outputs":[]}]}
